{"cells":[{"cell_type":"markdown","id":"d9790859","metadata":{},"source":["# Mandatory Assignment 6\n","## Submission Deadline: 15th March, 2022\n","\n","#### Reinforcement Learning and DecisionMaking Under Uncertainty IN-STK 5100/9100, Spring 2022, UiO\n","#### A. George, 1st March, 2022"]},{"cell_type":"markdown","id":"d1802df8","metadata":{},"source":["## Description:\n","In this exercise, we will implement Policy Iteration and Value Iteration. \\\n","Remember to **only fill in the marked gaps** and do not modify any other bits of code (except trying different parameters)!\\\n","There are experiments provided at the end with which you can **verify that your code is running correctly**.\n","\n","Let's start!\n","\n","#### Some Notation:\n","- $\\gamma$ discount factor\n","- $p(s',r | s,a)$ transition function, i.e., probability of ending up in state $s'$ with reward $r$ when executing action $a$ in state $s$.\n","- $V(s)$ the estimated value of a state $s$\n","- $Q(s,a)$ the estimated value of a state $s$ and action $a$"]},{"cell_type":"code","execution_count":95,"id":"60d6b938","metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","import random\n","import matplotlib.pyplot as plt\n","import math\n","from itertools import product"]},{"cell_type":"markdown","id":"bc8267f4","metadata":{},"source":["## 1) Action Values and Greedy Actions"]},{"cell_type":"markdown","id":"9260b16b","metadata":{},"source":["#### Find the state-action-value for a given state $s$ and action $a$ based on (given) state-values $V$:\n","$Q(s,a) = \\sum_{s',r} p(s',r|s,a)[r+\\gamma V(s')]$. \n","- **s,a**: both int, the state action pair (indices) to evaluate\n","- **p**: function taking 4 arguments, the transition function called as p(s_prime, r, s, a), where s_prime and s are states (indices), r is a reward value and a is an action (index)\n","- **rewards**: array, the possible reward values\n","- **gamma**: float, discount factor\n","- **V**: array, state-values for all states"]},{"cell_type":"code","execution_count":96,"id":"04c1bc33","metadata":{"trusted":true},"outputs":[],"source":["def state_action_value(s,a,p,rewards,gamma,V):\n","    n_states = np.shape(V)[0]       # the number of states\n","    Q = 0\n","    ### FILL IN THE GAP HERE:\n","    #   - find the state action value based on state values V as indicated in the description\n","    for s_prime,Vs_prime in enumerate(V):\n","        for r in rewards:\n","            Q += p(s_prime, r, s, a) * (r + gamma * Vs_prime)\n","            \n","    return Q"]},{"cell_type":"markdown","id":"a58e2652","metadata":{},"source":["#### Find a greedy action and it's value for a given state $s$ based on (given) state-values $V$:\n","Greedy action = $argmax_{a} \\sum_{s',r} p(s',r|s,a)[r+\\gamma V(s')]$ and\n","value of action = $max_{a} \\sum_{s',r} p(s',r|s,a)[r+\\gamma V(s')]$.\n","- **s**: int, the state (index) for which we find a greedy action\n","- **p**: function taking 4 arguments, the transition function called as p(s_prime, r, s, a), where s_prime and s are states (indices), r is a reward value and a is an action (index)\n","- **rewards**: array, the possible reward values\n","- **gamma**: float, discount factor\n","- **n_actions**: int, the number of actions\n","- **V**: array, state-values for all states"]},{"cell_type":"code","execution_count":97,"id":"4743932b","metadata":{"trusted":true},"outputs":[],"source":["def greedy_action(s, p, rewards, gamma, n_actions, V):\n","    action = 0\n","    value = 0\n","    ### FILL IN THE GAP HERE:\n","    #   - find a greedy action and it's value based on state values V as indicated in the description\n","    Q = np.zeros(n_actions)\n","    for a in range(n_actions):\n","        Q[a] = state_action_value(s, a, p, rewards, gamma, V)\n","\n","    action = np.argmax(Q)\n","    value = Q[action]\n","    \n","    return action, value"]},{"cell_type":"markdown","id":"6de971c6","metadata":{},"source":["## 2) Policy Evaluation"]},{"cell_type":"markdown","id":"df4cb981","metadata":{},"source":["#### Find (approximate) the state value function for a given policy $\\pi$:\n","Initialise the state values to be 0, i.e., $V(s)=0$ for all states $s$.  \n","Update the state values (in place) by using the Bellman Equation: $V(s) = \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s,a)[r+\\gamma V(s')]$.  \n","Update the state values until for none of the states the change in value is $\\geq \\theta$.\n","- **theta**: float, the termination criterion\n","- **gamma**: float, discount factor\n","- **rewards**: array, the possible reward values\n","- **p**: function taking 4 arguments, the transition function called as p(s_prime, r, s, a), where s_prime and s are states (indices), r is a reward value and a is an action (index)\n","- **pi**: $|\\mathcal{S}|\\times|\\mathcal{A}|$-matrix, the policy, i.e., pi[s][a] is the probability of performing action a in state s."]},{"cell_type":"code","execution_count":98,"id":"7b3648db","metadata":{"trusted":true},"outputs":[],"source":["def policy_evaluation(theta, gamma, rewards, p, pi):\n","    n_states = np.shape(pi)[0]      # the number of states\n","    n_actions = np.shape(pi)[1]     # number of actions\n","    V = np.zeros(n_states)          # the initial state values set to zero\n","    ### FILL IN THE GAP HERE:\n","    #   - implement a while loop with condition that the change in values \"delta\" is greater or equal theta\n","    #   - within the while loop update the state values and delta\n","    delta = theta\n","    while delta >= theta:\n","        V_new = np.zeros(n_states)\n","\n","        for s in range(n_states):\n","            for a in range(n_actions):\n","               V_new[s] = pi[s,a] * state_action_value(s, a, p, rewards, gamma, V)\n","\n","        delta = np.max(np.abs(V-V_new))\n","        V = np.copy(V_new)\n","    return V"]},{"cell_type":"markdown","id":"6758974b","metadata":{},"source":["## 3) Policy Iteration"]},{"cell_type":"markdown","id":"3ceabf75","metadata":{},"source":["#### Find an optimal (deterministic) policy and the optimal state values by Policy Iteration:\n","Initialise the policy to the uniform policy, i.e., the policy that selects actions uniformly at random.  \n","Iteratively, evaluate the policy by calling policy_evaluation() and find a greedy policy for the new values using greedy_action().  \n","Stop, once the policy is stable, i.e., does not change anymore.\n","- **theta**: float, the termination criterion\n","- **gamma**: float, discount factor\n","- **rewards**: array, the possible reward values\n","- **p**: a function taking 4 arguments, the transition function called as p(s_prime, r, s, a), where s_prime and s are states (indices), r is a reward value and a is an action (index)\n","- **n_actions**: int, the number of actions\n","- **n_states**: int, the number of actions"]},{"cell_type":"code","execution_count":99,"id":"8b3bb64a","metadata":{"trusted":true},"outputs":[],"source":["def policy_iteration(theta, gamma, rewards, p, n_actions, n_states):\n","    pi = np.zeros((n_states,n_actions))   # the policy\n","    V = np.zeros(n_states)                # the state values\n","    ### FILL IN THE GAP HERE:\n","    #   - initialise a policy pi (matrix) to be the uniform policy\n","    #   - implement a while loop with condition that the current policy is not stable\n","    #   - within the while loop \n","    #             - find the state values for the current policy using policy_evaluation()\n","    #             - find a greedy action for every state using greedy_action() and update the policy accordingly\n","    #             - update whether the policy is stable\n","    pi = np.ones((n_states,n_actions))/n_actions\n","    \n","    stable = False\n","    while stable is not True:\n","        pi_prime = np.zeros((n_states,n_actions))\n","        V_current = policy_evaluation(theta, gamma, rewards, p, pi)\n","\n","        for s in range(n_states):\n","            action,value = greedy_action(s, p, rewards, gamma, n_actions, V)\n","\n","            if value > V_current[s]:\n","                pi_prime[s,action] = 1\n","                V[s] = value            \n","        \n","        if np.array_equal(pi, pi_prime):\n","            stable = True\n","        \n","        pi = np.copy(pi_prime)\n","\n","    return pi, V        "]},{"cell_type":"markdown","id":"3c09830f","metadata":{},"source":["## 4) Value Iteration"]},{"cell_type":"markdown","id":"5fc200c4","metadata":{},"source":["#### Find an optimal (deterministic) policy and the optimal state values by Value Iteration:\n","Initialise the state values to be 0, i.e., $V(s)=0$ for all states $s$.  \n","Update the state values by using the Bellman Optimality Equation: $V(s) = \\max_{a} \\sum_{s',r} p(s',r|s,a)[r+\\gamma V(s')]$.  \n","Update the state values until for none of the states the change in value is $\\geq \\theta$.  \n","Find a greedy action for all states based on the final state values using greedy_action().\n","- **theta**: float, the termination criterion\n","- **gamma**: float, discount factor\n","- **rewards**: array, the possible reward values\n","- **p**: a function taking 4 arguments, the transition function called as p(s_prime, r, s, a), where s_prime and s are states (indices), r is a reward value and a is an action (index)\n","- **n_actions**: int, the number of actions\n","- **n_states**: int, the number of actions"]},{"cell_type":"code","execution_count":100,"id":"52734371","metadata":{"trusted":true},"outputs":[],"source":["def value_iteration(theta, gamma, rewards, p, n_actions, n_states):\n","    pi = np.zeros((n_states,n_actions))   # the policy\n","    V = np.zeros(n_states)                # the state values\n","    ### FILL IN THE GAP HERE:\n","    #   - initialise the state values to be zero for all states (already done)\n","    #   - implement a while loop with condition that the change in values \"delta\" is greater or equal theta\n","    #   - within the while loop update the state values and delta  \n","    #   - find a greedy action for every state and update the policy accordingly\n","    delta = theta\n","    while delta >= theta:\n","        V_new = np.zeros(n_states)\n","\n","        for s in range(n_states):\n","            for a in range(n_actions):\n","                Q = state_action_value(s,a,p,rewards,gamma,V)\n","                V_new[s] = max(V_new[s],Q)\n","            \n","        delta = np.max(np.abs(V-V_new))\n","        V = np.copy(V_new)\n","\n","    for s in range(n_states):\n","        action, value = greedy_action(s, p, rewards, gamma, n_actions, V)\n","        pi[s, action] = value\n","\n","    return pi, V     "]},{"cell_type":"markdown","id":"727aa0f3","metadata":{},"source":["## 5) Evaluation: Grid World"]},{"cell_type":"markdown","id":"2c7e196e","metadata":{},"source":["### Set up the grid world"]},{"cell_type":"markdown","id":"b4cdbf7a","metadata":{},"source":["We consider the following (hard-coded) grid world MDP in which the transitions are deterministic (see example on p.60 & p.65 in the Sutton&Barto book / lecture slides for more details).  "]},{"cell_type":"markdown","id":"7308978e","metadata":{},"source":["|    |    |    |    |    |\n","|----|----|----|----|----|\n","|  x |  A |  x |  B |  x |\n","|  x |  x |  x |  x |  x |\n","|  x |  x |  x |  B'|  x |\n","|  x |  x |  x |  x |  x |\n","|  x |  A'|  x |  x |  x |\n","\n","- **States** are the fields marked with x or A,B,A',B'\n","- **Actions** are to move right, down, left, up\n","- **Transitions**\n","    - The next state will be the one following the move, except \n","        - when bumping into the boundary, stay in the same state\n","        - when in A, any action takes you to A'\n","        - when in B, any action takes you to B'\n","    - The reward after executing an action is 0, except\n","        - when bumping into the boundary, the reward is -1\n","        - when going from A to A', the reward is +10\n","        - when going from B to B', the reward is +5\n","- **Discount Factor** $\\gamma = 0.9$\n"]},{"cell_type":"code","execution_count":101,"id":"2c4c282f","metadata":{"trusted":true},"outputs":[],"source":["gamma = 0.9              # discount factor\n","rewards = [-1, 0, 5, 10] # list of possible rewards\n","n_states = 25            # number of states \n","n_actions = 4            # number of actions"]},{"cell_type":"markdown","id":"34e3a7f0","metadata":{},"source":["For convenience, we number the actions as $0 = move\\, right$, $1 = move\\, down$, $2 = move\\, left$, $3 = move\\, up$.  \n","We also number the states as follows:\n","\n","|    |    |    |    |    |\n","|----|----|----|----|----|\n","|  0 |  1 |  2 |  3 |  4 |\n","|  5 |  6 |  7 |  8 |  9 |\n","| 10 | 11 | 12 | 13 | 14 |\n","| 15 | 16 | 17 | 18 | 19 |\n","| 20 | 21 | 22 | 23 | 24 |\n","\n","Thus, state A=1, A'=21,B=3,B'=13.\n","\n","The transitions can be hard-coded as follows:"]},{"cell_type":"code","execution_count":102,"id":"cd13bb64","metadata":{"trusted":true},"outputs":[],"source":["def p(sp,r,s,a): # output the probability of ending up in state sp with reward r when performing action a in state s\n","    if s == 1:\n","        if sp == 21 and r == 10:  # move from field A to A'\n","            return 1\n","        else:\n","            return 0\n","    if s == 3:\n","        if sp == 13 and r == 5:   # move from field B to B'\n","            return 1\n","        else:\n","            return 0\n","    column_s = s % 5\n","    row_s = (s-column_s)/5\n","    if row_s == 0 and s == sp and r == -1 and a == 3:     # bump into the upper boundary\n","        return 1\n","    if row_s == 4 and s == sp and r == -1 and a == 1:     # bump into the lower boundary\n","        return 1\n","    if column_s == 0 and s == sp and r == -1 and a == 2:  # bump into the left boundary\n","        return 1\n","    if column_s == 4 and s == sp and r == -1 and a == 0:  # bump into the right boundary\n","        return 1\n","    column_sp = sp % 5\n","    row_sp = (sp-column_sp)/5\n","    if column_s+1 == column_sp and row_s == row_sp and r == 0 and a == 0:   # move to the right\n","        return 1\n","    if column_s-1 == column_sp and row_s == row_sp and r == 0 and a == 2:   # move to the left\n","        return 1\n","    if column_s == column_sp and row_s+1 == row_sp and r == 0 and a == 1:   # move down\n","        return 1\n","    if column_s == column_sp and row_s-1 == row_sp and r == 0 and a == 3:   # move up\n","        return 1\n","    return 0 # all other scenarious are not feasible, i.e., have probability 0\n","    "]},{"cell_type":"markdown","id":"1cc472bf","metadata":{},"source":["### Run Policy & Value Iteration"]},{"cell_type":"markdown","id":"769f3f6f","metadata":{},"source":["For both methods we need to set a termination criterium, that determines how accurately we want to approximate the state value function of a policy / the optimal state value function."]},{"cell_type":"code","execution_count":103,"id":"4927cd97","metadata":{"trusted":true},"outputs":[],"source":["theta = 0.001           # value estimation accuracy / termination criterion"]},{"cell_type":"markdown","id":"a41d6309","metadata":{},"source":["To visualise a (deterministic) policy we can call the following function:"]},{"cell_type":"code","execution_count":104,"id":"6de3008f","metadata":{"trusted":true},"outputs":[],"source":["def print_policy(pi):\n","    n_states = np.shape(pi)[0]      # the number of states\n","    n_actions = np.shape(pi)[1]     # number of actions\n","    actions = np.argmax(pi, axis=1)\n","    unicodes =[]\n","    for s in range(n_states):\n","        if actions[s] == 0:\n","            unicodes.append(u\"\\u2192\")\n","        if actions[s] == 1:\n","            unicodes.append(u\"\\u2193\")\n","        if actions[s] == 2:\n","            unicodes.append(u\"\\u2190\")\n","        if actions[s] == 3:\n","            unicodes.append(u\"\\u2191\")\n","    print(\"+\" + \"---+\"*5)\n","    for s in range(0,n_states, 5):\n","        print((\"|\" + \" {} |\"*5).format(*[unicodes[x] for x in range(s, s + 5)]))\n","        print(\"+\" + \"---+\"*5)"]},{"cell_type":"markdown","id":"95d45913","metadata":{},"source":["We can now consider the output of Policy Iteration..."]},{"cell_type":"code","execution_count":105,"id":"6460db0b","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Optimal state values found by Policy Iteration (rounded):\n","[[ 9.  15.9 14.3 10.9  9.8]\n"," [ 8.1 14.3 12.9 11.6 10.4]\n"," [ 7.3 12.9 11.6 10.4  9.4]\n"," [ 6.6 11.6 10.4  9.4  8.5]\n"," [ 5.9 10.4  9.4  8.5  7.6]]\n","Optimal policy found by Policy Iteration:\n","+---+---+---+---+---+\n","| → | → | ← | → | ← |\n","+---+---+---+---+---+\n","| → | ↑ | ← | ← | ← |\n","+---+---+---+---+---+\n","| → | ↑ | ← | ← | ← |\n","+---+---+---+---+---+\n","| → | ↑ | ← | ← | ← |\n","+---+---+---+---+---+\n","| → | ↑ | ← | ← | ← |\n","+---+---+---+---+---+\n"]}],"source":["pi, V = policy_iteration(theta, gamma, rewards, p, n_actions, n_states)\n","V_grid = np.reshape(V, (5, 5))\n","V_grid = np.around(V_grid, decimals=1, out=None)\n","print('Optimal state values found by Policy Iteration (rounded):')\n","print(V_grid)\n","print('Optimal policy found by Policy Iteration:')\n","print_policy(pi) "]},{"cell_type":"markdown","id":"f7dd8a53","metadata":{},"source":["... and Value Iteration"]},{"cell_type":"code","execution_count":106,"id":"48b05b5e","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Optimal state values found by Value Iteration (rounded):\n","[[22.  24.4 22.  19.4 17.5]\n"," [19.8 22.  19.8 17.8 16. ]\n"," [17.8 19.8 17.8 16.  14.4]\n"," [16.  17.8 16.  14.4 13. ]\n"," [14.4 16.  14.4 13.  11.7]]\n","Optimal policy found by Value Iteration:\n","+---+---+---+---+---+\n","| → | → | ← | → | ← |\n","+---+---+---+---+---+\n","| → | ↑ | ← | ← | ← |\n","+---+---+---+---+---+\n","| → | ↑ | ← | ← | ← |\n","+---+---+---+---+---+\n","| → | ↑ | ← | ← | ← |\n","+---+---+---+---+---+\n","| → | ↑ | ← | ← | ← |\n","+---+---+---+---+---+\n"]}],"source":["pi, V = value_iteration(theta, gamma, rewards, p, n_actions, n_states)\n","V_grid = np.reshape(V, (5, 5))\n","V_grid = np.around(V_grid, decimals=1, out=None)\n","print('Optimal state values found by Value Iteration (rounded):')\n","print(V_grid)\n","print('Optimal policy found by Value Iteration:')\n","print_policy(pi) "]},{"cell_type":"markdown","id":"7921d489","metadata":{},"source":["### Verify your result:\n","Both Policy Iteration and Value Iteration should give the same results. You can find the optimal state values and a (non-deterministic) optimal policy on p.65 of the book from Sutton&Barto: http://www.incompleteideas.net/book/RLbook2018.pdf.\n","\n","* My state values are equal for the value iteration, but for policy iterations the values are different. I guess this is due to the stopping criterion is $\\pi = \\pi'$, and not $\\theta > \\max_{s\\in \\mathcal{S}} |v_{k+1}(s)-v_k(s)|$. The optimal actions are however equal."]},{"cell_type":"markdown","id":"19a223b4","metadata":{},"source":["## 6) Optional (not graded): Implement your own grid world"]},{"cell_type":"markdown","id":"e05e8671","metadata":{},"source":["If you like, you can implement your own grid world.  \n","- What is your intuition for an optimal policy / optimal state values for this grid world?  \n","- Apply Policy Iteration and/or Value Iteration to find an optimal policy / optimal state values.\n","- Do the results match your initial intuition? Why / why not?"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":5}
